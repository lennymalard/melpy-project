import numpy as np

from .functions import *
from .metrics import accuracy
from .optimizers import *
from .LiveMetrics import *

from math import sqrt
from copy import deepcopy           
import matplotlib.pyplot as plt
from tqdm import tqdm
import time
import pickle
import sys


class Sequential:
    """
    A class for creating and training Feedforward Neural Networks (FNN) or Convolutional Neural Networks (CNN).
    
    Attributes
    ----------
    train_inputs : ndarray
        Input data for training the model.
    train_input_batch : ndarray
        A batch of training inputs.
    train_targets : ndarray
        Target data corresponding to training inputs.
    train_target_batch : ndarray
        A batch of training targets.
    train_outputs : ndarray
        Final output predictions during the training phase.
    train_output_batch : ndarray
        A batch of outputs during training.
 
    val_inputs : ndarray
        Input data for validating the model (optional).
    val_input_batch : ndarray
        A batch of validation inputs.
    val_targets : ndarray
        Target data corresponding to validation inputs (optional).
    val_target_batch : ndarray
        A batch of validation targets.
    val_outputs : ndarray
        Final output predictions during the validation phase.
    val_output_batch : ndarray
        A batch of outputs during validation.
 
    predictions : ndarray
        Predictions generated by the model for a given input array.
 
    train_layers : list
        List of layers used during training.
    val_layers : list
        List of layers used during validation.
 
    train_loss : float
        Calculated loss for the training dataset.
    val_loss : float
        Calculated loss for the validation dataset.
    train_loss_history : list
        History of training loss values for each epoch.
    val_loss_history : list
        History of validation loss values for each epoch.
 
    train_accuracy : float
        Accuracy computed on the training dataset.
    val_accuracy : float
        Accuracy computed on the validation dataset.
    train_accuracy_history : list
        History of training accuracy values for each epoch.
    val_accuracy_history : list
        History of validation accuracy values for each epoch.
 
    train_cost_fn : callable
        Cost function used for training.
    val_cost_fn : callable
        Cost function used for validation.
 
    runtime : float
        Total time taken to train the model.
 
    optimizer : Optimizer object
        Optimizer instance used to update model parameters.
    validation : bool
        Indicates whether a validation dataset is used.
 
    Methods
    -------
    forward()
        Performs the forward pass for both training and validation layers.
    backward()
        Executes the backward pass to compute gradients for training layers.
    verbose(verbose : int, epoch : int, epochs : int, start_time : float)
        Displays training and validation metrics based on the specified verbosity level.
    fit(cost_fn : callable, epochs : int, batch_size : int, optimizer : callable, lr : float, momentum : float, verbose : int, live_metrics : int or LiveMetrics, extension : callable)
        Trains the model using the specified configurations and tracks metrics over epochs.
    predict(X : ndarray)
        Generates predictions for the input data `X`.
    results()
        Visualizes loss and accuracy curves for both training and validation phases.
    save_params(name : str)
        Saves the model's weights and biases to a file.
    load_params(path : str, parameters : dict)
        Loads model weights and biases from a file or dictionary.
    """
    def __init__(self, train_inputs, train_targets, val_inputs = None, val_targets = None):
        """
        Initializes the architecture with training and optional validation data.

        This constructor method sets up the necessary inputs and targets for both training 
        and validation (if provided). The `train_inputs` and `train_targets` are essential 
        for training the model, while `val_inputs` and `val_targets` are used for model 
        evaluation during training. If validation data is not provided, the model will 
        only be trained on the training data.

        Parameters
        ----------
        train_inputs : ndarray
            The input data used to train the model. 
        
        train_targets : ndarray
            The target labels or values corresponding to the `train_inputs`. 

        val_inputs : ndarray, optional
            The input data used for validation. Default is 'None'.

        val_targets : ndarray, optional
            The target labels or values corresponding to the `val_inputs`. Default is `None`.

        Returns
        -------
        None
        """
        self.train_inputs = train_inputs
        self.train_input_batch = None
        self.train_targets = train_targets
        self.train_target_batch = None
        self.train_outputs = None
        self.train_output_batch = None
        
        self.val_inputs = val_inputs
        self.val_input_batch = None
        self.val_targets = val_targets
        self.val_target_batch = None
        self.val_outputs = None
        self.val_output_batch = None
        
        self.predictions = None
        
        self.train_layers = []
        self.val_layers = []
        
        self.train_loss = 0.0
        self.val_loss = 0.0
        self.train_loss_history = []
        self.val_loss_history = []
        
        self.train_accuracy = 0.0
        self.val_accuracy = 0.0
        self.train_accuracy_history = []
        self.val_accuracy_history =  []
        self.train_cost_fn = None
        self.val_cost_fn = None
        
        self.__activation_functions__ = {"relu": ReLU(),
                                         "leaky_relu": LeakyReLU(),
                                         "sigmoid": Sigmoid(),
                                         "softmax": Softmax()}
        
        self.__cost_functions__ = {"binary_crossentropy": Binary_CrossEntropy(),
                                   "categorical_crossentropy": Categorical_CrossEntropy()}
        self.__is_trained__ = False
        self.runtime = 0.0
        self.optimizer = None
        self.validation = False
        
        if self.val_inputs is not None and self.val_targets is not None: 
            self.validation = True     
            
    def forward(self):
        """
        Performs the forward pass through the layers of the architecture for both 
        training and validation.

        This method computes the output for both the training and validation data. 
        During training, the input batch goes through the layers in sequence, 
        and the output is stored in `train_output_batch`. If validation data is 
        provided, the validation input batch goes through the validation layers 
        and the output is stored in `val_output_batch`.

        Returns
        -------
        None
        """
        self.train_layers[0].inputs = self.train_input_batch
        if self.validation is True: 
            self.val_layers[0].inputs = self.val_input_batch
        for i in range(len(self.train_layers)):
            if i+1 == len(self.train_layers):
                self.train_output_batch = self.train_layers[i].forward()
            else:
                self.train_layers[i+1].inputs = self.train_layers[i].forward()
            if self.validation is True: 
                if i+1 == len(self.val_layers):
                    self.val_output_batch = self.val_layers[i].forward()
                else:
                    self.val_layers[i+1].inputs = self.val_layers[i].forward()
                
    def predict(self,X):
        """
       Makes predictions for the given input `X` using the trained model.

       This method runs a forward pass with the provided input `X` and returns 
       the predicted output based on the current state of the trained network.

       Parameters
       ----------
       X : ndarray
           The input data for which predictions are to be made.

       Returns
       -------
       predictions : ndarray
           The predicted output for the given input `X`.
       """
        self.train_layers[0].inputs = X
        for i in range(len(self.train_layers)):
            if i+1 == len(self.train_layers):
                if isinstance(self.train_layers[i], Dropout) and \
                    isinstance(self.val_layers[i], Dropout):
                    self.val_layers[i].training = False
                    self.train_layers[i].training = False
                self.predictions = self.train_layers[i].forward()
                return self.predictions
            self.train_layers[i+1].inputs = self.train_layers[i].forward()
            
    def backward(self):
        """
        Performs the backward pass through the layers to compute gradients.

        This method calculates the gradients of the loss function with respect to 
        the parameters (weights and biases) of the network. The gradients are propagated backward through 
        each layer.

        Returns
        -------
        None
        """
        self.dX = self.train_cost_fn.derivative(self.train_target_batch, self.train_output_batch)
        for layer in reversed(self.train_layers):
                self.dX = layer.backward(self.dX)
        
    def verbose(self,verbose,epoch,epochs,start_time):
        """
        Prints training and validation metrics during training at specified intervals.

        This method provides feedback during training based on the verbosity level 
        chosen. It can print the loss and accuracy for both the training and validation 
        datasets, and also the training time. The method adjusts the print frequency 
        based on the `verbose` parameter.

        Parameters
        ----------
        verbose : int
            The verbosity level (0, 1, or 2).
            0 - No output.
            1 - Print at the end of each epoch.
            2 - Print periodically during training.
        epoch : int
            The current epoch number.
        epochs : int
            The total number of epochs.
        start_time : float
            The starting time of training to calculate the runtime.

        Raises
        ------
        ValueError
            If the `verbose` parameter is not 0, 1, or 2.

        Returns
        -------
        None
        """
        if verbose == 2:
            if epoch+1 < epochs:
                if self.validation is True:
                    print(f"[TRAINING METRICS] train_loss: {np.around(self.train_loss,5)} · " 
                          f"train_accuracy: {np.around(self.train_accuracy,5)}\n" +
                          f"[VALIDATION METRICS] val_loss: {np.around(self.val_loss,5)} · " +
                          f"val_accuracy: {np.around(self.val_accuracy,5)}\n\n")
                else:
                    print(f"[TRAINING METRICS] train_loss: {np.around(self.train_loss,5)} | " +
                          f"train_accuracy: {np.around(self.train_accuracy,5)}\n\n")
            elif epoch+1 == epochs:
                if self.validation is True:
                    self.runtime = time.time() - start_time
                    string1 = f"| [TRAINING METRICS] train_loss: {np.around(self.train_loss,5)} · " + \
                                f"train_accuracy: {np.around(self.train_accuracy,5)} |"
                    string2 = f"| [VALIDATION METRICS] val_loss: {np.around(self.val_loss,5)} · " + \
                                f"val_accuracy: {np.around(self.val_accuracy,5)} |"
                    string1_length = len(string1)
                    string2_length = len(string2)
                    print("\n" + string1_length * "-" + "\n" + string1 + "\n" +
                          string1_length * "-" + "\n" + \
                          string2+ (string1_length-string2_length-1) * " " + "\n" +
                          string1_length * "-")
                    print(f"{round(self.runtime, 5)} seconds")
                else:
                    self.runtime = time.time() - start_time
                    string = f"| [TRAINING METRICS] train_loss: {np.around(self.train_loss,5)} · "  + \
                             f"train_accuracy: {np.around(self.train_accuracy,5)} |"
                    string_length= len(string)
                    print("\n" + string_length * "-" + "\n" + string + "\n" + string_length * "-")
                    print(f"{round(self.runtime, 5)} seconds")
        elif verbose == 1:
            if epoch+1 == epochs:
                if self.validation is True:
                    string1 = f"| [TRAINING METRICS] train_loss: {np.around(self.train_loss,5)} · " + \
                                f"train_accuracy: {np.around(self.train_accuracy,5)} |"
                    string2 = f"| [VALIDATION METRICS] val_loss: {np.around(self.val_loss,5)} · " + \
                                f"val_accuracy: {np.around(self.val_accuracy,5)} |"
                    string1_length = len(string1)
                    string2_length = len(string2)
                    print("\n" + string1_length * "-" + "\n" + string1 + "\n" + string1_length * "-" + "\n" + 
                          string2 + (string1_length-string2_length-1) * " " + "\n" + string1_length * "-")
                else:
                    string = f"| [TRAINING METRICS] train_loss: {np.around(self.train_loss,5)} · " + \
                            f"train_accuracy: {np.around(self.train_accuracy,5)} |"
                    string_length = len(string)
                    print("\n" + string_length * "-" + "\n" + string + "\n" + string_length * "-")
        elif verbose == 0:
            return 
        else:
            raise ValueError("invalid value for 'verbose'")
    
    def fit(self, cost_fn, epochs = 1000, batch_size=None, optimizer = SGD(), lr = 0.1, momentum = None, verbose = 1, live_metrics = None, extension = None):
        """
        Trains the model using the provided cost function, optimizer, and other parameters.

        This method performs the training process for the neural network. It includes 
        forward and backward passes, loss and accuracy computation and parameter updates 
        using the chosen optimizer. During training, it also keeps track of the loss and 
        accuracy for both the training and validation datasets (if validation data is provided).

        Parameters
        ----------
        cost_fn : melpy.functions
            The cost function to use for training (e.g., "binary_crossentropy").
        epochs : int, optional
            The number of epochs for training. The default is 1000.
        batch_size : int, optional
            The number of samples per batch. If None, the entire dataset is used for each pass.
        optimizer : melpy.optimizers, optional
            The optimizer to use (e.g., SGD, Adam). The default is SGD().
        lr : float, optional
            The learning rate for the optimizer. The default is 0.1.
        momentum : int, optional
            The momentum to use for the optimizer. The default is None.
        verbose : int, optional
            The verbosity level for printing metrics during training. Default is 1.
        live_metrics : int or melpy.LiveMetrics.LiveMetrics, optional
            Controls the plotting of live metrics during training. Default is None.
        extension : callable, optional
            A custom function to extend training functionality. The default is None.

        Raises
        ------
        ValueError
            If some types or values are invalid.

        Returns
        -------
        None
        """
        if self.validation is True:
            self.val_layers = deepcopy(self.train_layers)
        
        if live_metrics is None:
            live_metrics = LiveMetrics(type=0)
        elif live_metrics is not None and isinstance(live_metrics, LiveMetrics) == False:
            live_metrics = LiveMetrics(type=live_metrics)

        if live_metrics.type not in [-1,0,1,2]:
            raise ValueError("invalid value for 'type'")

        if live_metrics.row_select != "full" and live_metrics.row_select != "limited":
            raise ValueError("invalid value for 'live_metrics.row_select'")
        
        if type(live_metrics.f1) != int:
            raise ValueError("invalid value for 'live_metrics.f1'")
        
        if type(live_metrics.f2) != int:
            raise ValueError("invalid value for 'live_metrics.f2'")
            
        self.__is_trained__ = True
        
        start_time = time.time()
        
        self.optimizer = optimizer
        self.optimizer.learning_rate = lr
        self.optimizer.momentum = momentum
        
        if batch_size is None:
            steps = 1
            val_batch_size = 1
        else:
            steps = self.train_inputs.shape[0] // batch_size
            if steps*batch_size < self.train_inputs.shape[0]:
                steps+=1
                
        if self.validation is True:        
            val_batch_size =  self.val_inputs.shape[0] // steps 
          
        train_cost_functions = deepcopy(self.__cost_functions__)
        for name,function in train_cost_functions.items():
            if cost_fn == name:
                self.train_cost_fn = function
                break
            
        if self.validation is True:
            val_cost_functions = deepcopy(self.__cost_functions__)
            for name,function in val_cost_functions.items(): 
                if cost_fn == name:
                    self.val_cost_fn = function
                    break
        
        if live_metrics.type == -1:
            update = 50
            figure = plt.figure()
        elif live_metrics.type == 0:
            update = 1
            figure = None
        elif live_metrics.type == 1 or live_metrics.type == 2: 
            update = 25
            figure = plt.figure()
        
        tqdm_epochs = False
        tqdm_steps = False
        
        if verbose == 1:
            tqdm_epochs = True            
        else:
            tqdm_steps = True
        
        for epoch in (epoch_bar := tqdm(range(epochs), disable=not tqdm_epochs)):
            epoch_bar.set_description(f"Epoch [{epoch+1}/{epochs}]")
            
            train_accumulated_loss = 0
            train_accumulated_accuracy = 0
            
            val_accumulated_loss = 0
            val_accumulated_accuracy = 0
            
            for step in (step_bar :=tqdm(range(steps), disable=not tqdm_steps)):
                step_bar.set_description(f"Epoch [{epoch+1}/{epochs}]")
                
                if batch_size is None:
                  self.train_input_batch = self.train_inputs
                  self.train_target_batch = self.train_targets
                  
                  if self.validation is True:
                      self.val_input_batch = self.val_inputs
                      self.val_target_batch = self.val_targets
                else:
                    self.train_input_batch = self.train_inputs[step*batch_size:(step+1)*batch_size]            
                    self.train_target_batch = self.train_targets[step*batch_size:(step+1)*batch_size]
                    
                    if self.validation is True:
                        self.val_input_batch =  self.val_inputs[step*val_batch_size:(step+1)*val_batch_size]                
                        self.val_target_batch = self.val_targets[step*val_batch_size:(step+1)*val_batch_size]
                
                self.forward() 
                self.backward()
                
                for i in range(len(self.train_layers)):
                    self.train_layers[i] = self.optimizer.update_params(self.train_layers[i])
                    
                    if self.validation is True:
                        if isinstance(self.val_layers[i], Dense):
                            self.val_layers[i].weights = self.train_layers[i].weights
                            self.val_layers[i].biases = self.train_layers[i].biases
                        elif isinstance(self.val_layers[i], Convolution2D):
                            self.val_layers[i].filters = self.train_layers[i].filters
                    
                train_accumulated_loss += self.train_cost_fn.loss(self.train_target_batch, self.train_output_batch)
                train_accumulated_accuracy += accuracy(self.train_target_batch, self.train_output_batch)
                
                if self.validation is True:
                    val_accumulated_loss += self.val_cost_fn.loss(self.val_target_batch, self.val_output_batch)
                    val_accumulated_accuracy += accuracy(self.val_target_batch, self.val_output_batch)
             
            self.train_loss = train_accumulated_loss/steps
            self.train_accuracy = train_accumulated_accuracy/steps
            
            if epoch % update == 0 or epoch == 1:
                self.train_loss_history.append(self.train_loss)
                self.train_accuracy_history.append(self.train_accuracy)
                
            if self.validation is True:
                self.val_loss = val_accumulated_loss/steps
                self.val_accuracy = val_accumulated_accuracy/steps
                
                if epoch % update == 0 or epoch == 1:
                    self.val_loss_history.append(self.val_loss)
                    self.val_accuracy_history.append(self.val_accuracy)
                    
                    
            if live_metrics.type != 0:
                if epoch % update == 0:
                    live_metrics.run(self, figure)
                        
            self.verbose(verbose, epoch, epochs, start_time)
                    
            if callable(extension):
                extension()
        
        self.train_outputs = self.predict(self.train_inputs)
        
        if self.validation is True:
            self.val_outputs = self.predict(self.val_inputs)
        
    def results(self):
        """
        Plots the loss and accuracy evolution over epochs.

        This method generates plots to visualize the progress of the training 
        process, showing how the loss and accuracy change for both training 
        and validation datasets (if validation data is provided). This helps 
        assess whether the model is learning and if it is overfitting.

        Raises
        ------
        RuntimeError
            If the model has not been trained yet.
        
        Returns
        -------
        None
        """
        if self.__is_trained__ == True:
        
            figure,axs = plt.subplots(1,2)
            
            axs[0].set_title("Loss Evolution")
            axs[0].set_xlabel("Epoch")
            axs[0].set_ylabel("Loss")
            axs[0].plot(self.train_loss_history, label="training dataset")
            if self.validation is True:
                axs[0].plot(self.val_loss_history, label="validation dataset")
            
            axs[1].set_title("Accuracy Evolution")
            axs[1].set_xlabel("Epoch")
            axs[1].set_ylabel("Accuracy")
            axs[1].plot(self.train_accuracy_history, label="training dataset")
            if self.validation is True:
                axs[1].plot(self.val_accuracy_history, label="validation dataset")
                
            axs[0].legend()
            axs[1].legend()
            
            plt.show()
        else:
            raise RuntimeError("model not trained")
            
    def save_params(self, name="parameters"):
        """
        Saves the trained model parameters (weights and biases) to a file.

        This method saves the weights and biases of all the layers of the trained 
        model into a pickle file. This allows the model to be restored later for 
        further use or evaluation.

        Parameters
        ----------
        name : str, optional
            The name of the file to save the parameters. Default is "parameters".

        Raises
        ------
        RuntimeError
            If the model has not been trained yet.

        Returns
        -------
        parameters : dict
            A dictionary containing the model's weights and biases.
        """
        if self.__is_trained__ == True:
            parameters = {"weights": None, "biases": None}
            weights = []
            biases = []
            for layer in self.train_layers:
                if isinstance(layer, Dense):
                    weights.append(layer.weights)
                    biases.append(layer.biases)
                elif isinstance(layer, Convolution2D):
                    weights.append(layer.filters)
                    biases.append(None)
                else:
                    weights.append(np.array(0.0))
                    biases.append(np.array(0.0))
            parameters["weights"] = weights
            parameters["biases"] = biases
            with open(name+".pkl", 'wb') as f:
                pickle.dump(parameters, f)
            return parameters
        raise RuntimeError("model not trained")
        
    def load_params(self, path="parameters.pkl", parameters = None):
        """
        Loads the model parameters (weights and biases) from a file or a given dictionary.

        This method loads the weights and biases from a pickle file or a dictionary and 
        restores them to the corresponding layers in the model. This is useful for continuing 
        training or evaluating a previously trained model.

        Parameters
        ----------
        path : str, optional
            The file path from which to load the parameters. Default is "parameters.pkl".
        parameters : dict, optional
            A dictionary containing the model's weights and biases. Default is None.

        Raises
        ------
        TypeError
            If the type of `parameters` is not a dictionary.

        Returns
        -------
        None
        """
        if type(parameters) == dict or parameters is None:
            if parameters is None:
                with open(path, 'rb') as f:
                    parameters = pickle.load(f)
            for i in range(len(self.train_layers)):
                if isinstance(self.train_layers[i], Dense):
                    self.train_layers[i].weights = parameters["weights"][i]
                    self.train_layers[i].biases = parameters["biases"][i]
                elif isinstance(self.train_layers[i], Convolution2D):
                    self.train_layers[i].filters = parameters["weights"][i]
                    self.train_layers[i].biases = parameters["biases"][i]
        else:
            raise TypeError("invalid type for 'parameters'")
            